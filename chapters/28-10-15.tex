До сих пор было $\pi(\theta)$ - априорное распределение и $\braces{P_\theta, \theta \in \Theta}$
\subsection*{Байесовскийесовское задание гладкости}
Задается ковариационная функция $K(X, X')$, если $K(X, X') \to 0, X' \to X$, то процесс «гладкий» (???). У нас процесс гауссовский, решаем задачу регрессии $y_i = f(x_i) + \epsilon_i$, предполагаем, что $f$ - реализация гауссовского поля, $\epsilon$ - белый шум. \\
Пусть есть какой-то параметрический класс ковариационных функций (у нас это $\sim exp\braces{-dist(X, X')^2}$).
Существует теорема о нормальной корреляции:
$$ \begin{array}{c} y_1 \\ y_2 \end{array} \in N 
\begin{pmatrix} 
\mu_1 & \Sigma_{11} & \Sigma_{12} \\ 
\mu_2 & \Sigma_{21} & \Sigma_{22}	
\end{pmatrix}$$
$$ \Rightarrow Law(y_2|y_1) = N(\mu_1 + \Sigma_{21} \Sigma_{11}^{-1} (y_1-\mu_1), \Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})  $$

$$  Law(y(x)|y(x_1), \ldots, y(x_n)) \sim N(\hat{f}(x), \hat{\sigma}^2(x)) $$
$$ \hat{f}(x) = \sum \alpha_i K(x, x_i) $$
$$ \hat{f}(x) = k(x) K y \ \ (k(x) = K(x,x), \ K=K(x_i, x_j)_{i,j})$$
В отличие от ядерной регрессии, можно оценить дисперсию и построить доверительное множество
$$ \hat{\sigma}^2(x) = K(x,x) - k(x)K^{-1}k(x)^T $$
Оценка параметров ковариационной функции - максимальное правдоподобие

$$K(x,x') = \sigma^2 K(x,x'|\theta)$$
Из белого шума:
$$ K(x,x') = K(x,x') + \sigma \delta(x,x') $$

$K = K + \sigma^2I$ - имеет свойство регуляризации
$$ K = \sigma^2 K_{\theta}(x_i, x_j) $$